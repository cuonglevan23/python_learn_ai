{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ea986217",
   "metadata": {},
   "source": [
    "# Thống kê các phép do simlilarity\n",
    "## mô tả:\n",
    "-  biến ngẫu nhiên, phân biệt và phân tích đặc điểm của biến ngẫu nhiên rời rạc và liên tục - hai mô hình hóa cơ bản trong xử lý dữ liệu định lượng.\n",
    " - giá trị ky vọng(Mean): trình bày công thức và ý nghĩa thống kê của trung bình cộng trong việc đại diện cho xu hướng trung tâm của một phân phối\n",
    " - phương sai(variance) và độ lệch chuẩn(standard deviation): là thước đó chính xác độ phân tần của dữ liệu so với trung bình các công thức định lượng cụ thể.\n",
    " - Hiệp phương sai (Covariance) và hệ số tương quan (Correlation): là thước đo mối quan hệ giữa hai biến ngẫu nhiên. đóng vai trò quan trọng trong phân tích đa biến\n",
    " - các phân phối xác suất quan trọng: \n",
    "  - Bernoulli: Mô hình hóa các hiện tượng nhi phân(có/không, đúng/sai)\n",
    "  - Uniform: đại diện cho phân phối đồng đều - nơi mỗi kết quả có xác suất như nhau.\n",
    "  - Normal: Phân phối chuẩn - nền tảng của nhiều mô hình phân tích thống kế và học máy\n",
    "  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c116eccd",
   "metadata": {},
   "source": [
    "# Basic Probability \n",
    "-  bài tập 1: kết quả của đoạn chương trình tính mean sau đây là cho data x = {2, 0, 2, 2, 7, 4, −2, 5, −1, −1} \n",
    "    hãy hoàn thiện fuction compute_mean() để tính mean u của x để cho \n",
    "    - data X = {x1,....,xn}\n",
    "    - mean u = (x1 + x2 + ... +xn)/n\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "acf17ef9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean of X is:  1.8\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "def compute_mean(X):\n",
    "    return np.mean(X)\n",
    "X = [2, 0, 2, 2, 7, 4, -2, 5, -1, -1]\n",
    "\n",
    "print(\"mean of X is: \", compute_mean(X))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5402c52e",
   "metadata": {},
   "source": [
    "- bài tập 2: cho data x = {1,5,4,4,9,13} hãy hoàn thiện fuction compute_median() để tìm median m của X đã cho \n",
    "- data X = {1,5,4,4,9,13}\n",
    "- median = \n",
    "  sort X -> S(tăng dần)\n",
    "  if N is chẵn: median = (S[N/2] + S[N/2 + 1]) / 2\n",
    "  if N is lẻ: median = S[N/2]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "73941763",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1  4  4  5  9 13]\n",
      "Median:  4.5\n"
     ]
    }
   ],
   "source": [
    "def compute_median(X):\n",
    "    size = len(X)\n",
    "    X = np.sort(X)\n",
    "    print(X)\n",
    "    if size % 2 == 1:\n",
    "        return X[size // 2]\n",
    "    else:\n",
    "        return (X[size // 2 - 1] + X[size // 2]) / 2\n",
    "X = [1, 5, 4, 4, 9,13]\n",
    "print(\"Median: \", compute_median(X))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db58b8c6",
   "metadata": {},
   "source": [
    "- bài 3: kết quả của đoạn chương trình tính variance và standard deviation sau đây là\n",
    "cho data X = X = {171, 176, 155, 167, 169, 182} hãy hoàn thiện function compute_std() để tìm standard deviation o của X đã cho \n",
    "- Data X = {171, 176, 155, 167, 169, 182}\n",
    "- Mean = u = 1/n * sum(X)\n",
    "- Variance = 1/n * sum((x - u)^2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e11b398",
   "metadata": {},
   "outputs": [],
   "source": [
    "- Standard Deviation = sqrt(Variance)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2c34119b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.32666399786453\n"
     ]
    }
   ],
   "source": [
    "def compute_std(X):\n",
    " mean = compute_mean(X)\n",
    " variance = 0\n",
    " for x in X:\n",
    "  variance += (x - mean) ** 2\n",
    " return (variance / len(X)) ** 0.5\n",
    "X = [171, 176, 155, 167, 169, 182]\n",
    "print(compute_std(X))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f83f9d67",
   "metadata": {},
   "source": [
    "- bài 4 kết quả của đoạn chương trình tính corelation coefficient sau đây là:\n",
    " - cho data X = { −2, −5, −11, 6, 4, 15, 9} và Y = { 4, 25, 121, 36, 16, 225, 81}\n",
    " - Hoàn thiện function compute_correlation_coefficient() để tìm corelation coefficient của X và Y đã cho:\n",
    " - Random variable X,Y: X = {x1, x2, x3, ...} và Y = {y1, y2, y3, ...}\n",
    " - Corelation coefficient:  pxy = E[(X-E(X))(Y-E(Y))]/sqrt(E[(X-E(X))^2]E[(Y-E(Y))^2])\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3f06a2d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correlation:  0.42026333096027263\n"
     ]
    }
   ],
   "source": [
    "def compute_correlation_coefficient(X, Y):\n",
    "    N = len(X)\n",
    "    numerator = N * sum(X * Y) - sum(X) * sum(Y)\n",
    "    denominator = np.sqrt((N * sum(X ** 2) - sum(X) ** 2) * (N * sum(Y ** 2) - sum(Y) ** 2))\n",
    "    return numerator / denominator\n",
    "\n",
    "    \n",
    "X = np.asarray([-2, -5, -11, 6, 4, 15, 9])\n",
    "Y = np.asarray([4, 25, 121, 36, 16, 225, 81])\n",
    "print(\"Correlation: \", compute_correlation_coefficient(X,Y))\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96c8d0f9",
   "metadata": {},
   "source": [
    "# Vector database cho các ứng dụng AI\n",
    "- giới thiệu: trong bối cảnh nghệ phát triển, Lượng dữ liệu được tạo ra mỗi ngày đang chóng mặt, dữ liệu này không chỉ đa dạng về định dạng văn bản, hình ảnh, video, đến dywx liệu từ các thiết bị cảm biến(sensor data). và các file ghi hoạt động (log files) mà còn có các yêu cầu xử lý khác nhau về tốc độ, quy mô và tính nhất quán \n",
    "-  thách thức này đặt ra câu hỏi quan trọng làm sao để lưu trữ và truy xuất hiệu quả khối lượng dữ liệu đa dạng này? liệu các giải pháp cơ sở dữ liệu truyền thống có đủ khả năng đáp ứng nhu cầu của thời đại hiện đại? câu trả lời đăch biệt trở nên phức tạp khi chúng ta bước vào kỳ nguyên AI với sự bùng nổ của machine learning và deep learning một loại dữ liệu hoàn toàn mới đang trở nên cực kỳ quan trọng , Vector Embeddings - những biểu diễn số học của thông tin phức tạp như văn bản, hình ảnh, âm thanh, video, và thậm chí là các dữ liệu từ các thiết bị cảm biến(sensor data). lúc này giải pháp truyền thống như RDBMS(release database management system) không còn hiệu quả, bộc lộ nhiều hạn chế khi xử lý dữ liệu đa phương tiện, trong khi NoSQL gặp nhiều khó khăn với các truy vấn phức tạp.\n",
    "- Đây là lúc Vector Database ra đời, trờ thành giải pháp chuyên biệt giúp xử lý hiệu quả các tác vụ tìm kiếm tương tự (similarity seacher), matching và recommendation mà các hệ quản trị dữ liệu truyền thống như SQL hay NoSql không được thiết kế để tối ưu.\n",
    "- Trong nội dung tiếp theo chúng ta sẽ khám pháp chi tiết về Vector database cách chúng hoạt động, các công cụ phổ biến hiện nay và ứng dụng thực tiễn qua hai project cụ thể: - nhân dịen khuôn mặt để điểm danh nhân viên\n",
    "- hệ thống gợi ý cocktail dựa trên khẩu vị người dùng"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf402a67",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e00dde94",
   "metadata": {},
   "source": [
    "# INDEXING trong vector database\n",
    "- sau khi dữ liệu đã được chuyển đổi thành vector embedding, bước tiếp theo trong vector database là indexing - xây dựng chỉ mục giúp tối hóa khả năng truy vấn tìm kiếm gần đúng (Approximate Nearest Neighbors - ANN) với tốc độ nhanh và chi phí hợp lý ngay cả trên dữ liệu lớn\n",
    "- vậy tại sao chúng ta cần indexing?\n",
    "  - với hàng triệu đến hàng tỷ vector, việc so sánh trực tiếp(brute-force) không thể thực hiện được do chi phí tính toán cao.\n",
    "  - index giúp tổ chức các vector thông minh để chỉ cần tìm kiếm trong một phàn nhỏ không gian vector nhưng vẫn đảm bảo độ chính xác cao.\n",
    "-  trong phạm vị bàu này chúng ta sẽ tìm hiểu 3 cách indexing dữ liệu cơ bản đến nâng cao\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe15df93",
   "metadata": {},
   "source": [
    "# Flat Index\n",
    "- Flat index là môt tên gọi khác của tìm kiếm brute force. tất cả các vector được lữu trũ trong 1 cấu trúc chỉ mục duy nhất mà không có bất kù tổ chức phân cấp nào\n",
    "- có phần giống với KNN Flat Index là cách tiếp cận đơn giản nhất: vector database sẽ so sánh vector truy vấn với tất cả các vector đã được lưu trữ bằng một độ do khoảng cách (L2, Cosine, Dot product) sau đó trả về k vector gần nhất\n",
    "- đặc điểm của phương pháp này là:\n",
    "  -  chính xác tuyệt đối: vì do sánh toàn bộ vector dataset\n",
    "  - không cần buil index phức tạp\n",
    "  - chi phí tính toán cao nếu dataset lớn (hàng triệu vector trở lên)\n",
    "- Chúng ta sẽ dùng phương pháp này khi dataset(vài nghìn đến vài trăm nghìn vector) hoặc khi cần đảm bảo độ chính xác tuyệt đối và không giới hạn về tài nguyên "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bc4c67c",
   "metadata": {},
   "source": [
    "# Inverted File Index - IVF\n",
    "- IVF là một trong nhữ kỹ thuật lập chỉ mục đơn giản và trực quan nhất. mặc dù thường dc sử dụng trong các hệ thống truy xuất văn bản. nó có thể được điều chỉnh cho cơ sở dữ liệu vector để tìm kiếm xấp xỉ lận cận gần nhất\n",
    "- vậy ivf hoạt động thế nào:\n",
    "  - sử dụng các thuật toán phân cum để chia tất cả các vector trong dataset thành các vùng khác nhau (cluster).\n",
    "  - kết quả là, mỗi cluster có một trọng tâm (centroid) tương ứng và mỗi vector chỉ được liên kết với một cluster tương ứng với centroid gần nhất của nó, do đó mỗi centroid duy trì thông tin về tất cả các vector tthuocj về phân vùng của nó\n",
    "  - khi truy vấn thay vì tìm kiếm vector gần nhất chúng ta chỉ cần centroid gần nhất với vector truy vấn và lúc này chúng ta chỉ cần tìm các vecyor gần nhất thuộc centroid của cluster đó\n",
    "- giả sử chúng ta có N vector và mỗi vector có D chiều độ phức tạp sẽ là 0(N D) để tìm vector gần nhất đối với Flat Index. so với IVF chúng ta có K trong tâm tổng cộng N vector, mỗi vector có D chiều và các vector được phân bổ đều trên tất cả các phân vùng khi độ phức tạp sẽ là 0(kD + N Dk)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dea52af",
   "metadata": {},
   "source": [
    "# Product Quantization (PQ)\n",
    "- Ý tưởng: về lượng tử hóa nói chung đề cập đến việc nén dữ liệu trong khi vẫn bảo toàn thông tin gốc. Chúng ta sẽ đi phần đầu là cách PQ tổ chức trong database trước\n",
    " - Vector gốc có d chiều được chia thành m segment\n",
    " - với mỗi đoạn thuật K-means được áp dụng để lượng hóa và tạo ra 1 codebook - 1 tập hợp các centroid đại diện cho các giá trị có thể của đoạn đó. thay vì lưu giá trị thực, ta lưu chỉ số của centroid tương ứng.\n",
    " - Kết quả: vector gốc d chiều có thể chỉ còn là một chuỗi các chỉ số codebook , giúp giảm kích thước từ vài kb xuống hcir còn bài bytes\n",
    "- với Vector truy vấn Q, ta chia Q bằng cách tương tự, rồi tính khoảng cách từ từng segment của Q tới các centroid tương ứng, tạo nên một ma trận khoảng cách Sau đó với mỗi PQ code(vector trong database đã được mã hóa) và cộng lại để ước tính khoảng cách giữa Q và các vector trong DB. Vector có tổng distance nhỏ nhất được xem là gần nhất."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
